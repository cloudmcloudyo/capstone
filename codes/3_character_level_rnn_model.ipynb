{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was ran on the Kaggle notebook. Credit to Patrick DeKelly https://www.kaggle.com/valkling/pythonicpythonscript4making-seinfeld-scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras as K\n",
    "import random\n",
    "import sqlite3\n",
    "\n",
    "from keras.layers import Input, Dropout, Dense, concatenate, Embedding\n",
    "from keras.layers import Flatten, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba7cb1f576a22127ba3f693c034ab0c0cfd8812e"
   },
   "source": [
    "Originally, I tried putting both the directions and dialogue into the text. However, since there are so many directions this ends up making a script that is mostly stuff like \"cut to a picture of a man in the street.\" or \"cut to stock video of a train\" ect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/for_train.txt\"\n",
    "text_file = open(path, 'r').read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b6bbcdeeefd738beaaf288fc37f1839d61ebc12"
   },
   "source": [
    "# Prep the Text for the RNN\n",
    "Next we will prepare an index of every unique character in our text. We are only getting rid of capitalization for simplicity, but still keeping all special characters. This will give us an output that retains the punctuation and format of the original. \n",
    "\n",
    "anything ~1MB+ is great\n",
    "\n",
    "While we could work with the every Seinfeld script, it ends up being a lot of data to go through within the time limit. As such, I added an if block to limit the text data to just the first half million characters. Using more text and training longer is a valid option for improving the output with more training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "e16a3a80940fb71572b065010771c5248c834e4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '\\\\', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¿', 'é']\n"
     ]
    }
   ],
   "source": [
    "Text_Data = text_file.lower()\n",
    "\n",
    "if len(Text_Data) > 500000:\n",
    "    Text_Data = Text_Data[:500000]\n",
    "\n",
    "charindex = list(set(Text_Data))\n",
    "charindex.sort() \n",
    "print(charindex)\n",
    "\n",
    "np.save(\"charindex.npy\", charindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jerry: you know, why we\\'re here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about \"we should go out\"? this is what they\\'re talking about...this whole thing, we\\'re all out now, no one is home. not one person here is home, we\\'re all out! there are people tryin\\' to find us, they don\\'t know where we are. (imitates one of these people \"tryin\\' to find us\"; pretends his hand is a phone) \"did you ring?, i can\\'t find him.\" (imitates other person on phone) \"where did he go?\" (the first person again) \"he didn\\'t tell me where he was going\". he must have gone out. you wanna go out: you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...there you\\'re staring around, whatta you do? you go: \"we gotta be getting back\". once you\\'re out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, it\\'s my feeling, you\\'ve gotta go. (pete\\'s luncheonette. jerry and george are sitting at a table.)\\njerry: seems to me, that button is in the worst possible spot. (talking about george\\'s shirt) the second button literally makes or breaks the shirt, look at it: it\\'s too high! it\\'s in no-man\\'s-land, you look like you live with your mother.\\ngeorge: are you through? (kind of irritated)\\njerry: you do of course try on, when you buy?\\ngeorge: yes, it was purple, i liked it, i don\\'t actually recall considering the buttons.\\njerry: oh, you don\\'t recall?\\ngeorge: (pretends he\\'s talking into a microphone) uh, no, not at this time.\\njerry: well, senator, i just like to know, what you knew and when you knew it. (a waitress approaches the table)\\nwaitress: mister seinfeld. (she pours coffee in his cup) mister costanza. (she wants to pour coffee, but george stops her)\\ngeorge: are, are you sure this is decaf? where\\'s the orange indicator?\\nwaitress: it\\'s missing'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text_Data[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "39df6021cb3006a38b59108b116e9514a8015fa8"
   },
   "source": [
    "# Create Sequences\n",
    "In a nutshell, this model will look at the last 75 characters in the script and attempt to predict the 76th. Our X variable will be a 75 character sequence and our Y variable will be the 76th character. This block chops the text data into such sequences of characters. \n",
    "\n",
    "Note that this part also tokenizes the characters, which is to say it replaces each character with a number that corresponds to it's index in charindex. This is why it is important to save a copy of the charindex with your model just in case. We will need it to decode our predictions later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "ff8898a2997216697ffdb6ab6b6a98e77355b431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.4 s, sys: 565 ms, total: 58 s\n",
      "Wall time: 58.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CHARS_SIZE = len(charindex)\n",
    "SEQUENCE_LENGTH = 75\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for i in range(0, len(Text_Data)-SEQUENCE_LENGTH, 1 ): \n",
    "    X = Text_Data[i:i + SEQUENCE_LENGTH]\n",
    "    Y = Text_Data[i + SEQUENCE_LENGTH]\n",
    "    X_train.append([charindex.index(x) for x in X])\n",
    "    Y_train.append(charindex.index(Y))\n",
    "\n",
    "X_train = np.reshape(X_train, (len(X_train), SEQUENCE_LENGTH))\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0847d548defb45832406ac64fa3873749fe2b5d"
   },
   "source": [
    "# Create the Model\n",
    "The model uses 3 LSTMs stacked on top of each. Adding another LSTM layer and/or running it a lot longer or in multiple session will give better results. However, the 3 LSTM should do fine in 6 hour and adding the loopbreaker to our code later will make even under trained models give good results. Also note that we are using CuDNNLSTMs. If you don't know what that is, it is a special LSTM layer specially made for NIVDA GPUs. These function the same as regular LSTM layers but are automatically optimised for the GPU. You lose some customization with these layers but they work roughly twice as fast as regular LSTMs layers if conditions are right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f6498ca0985caf5a773a5e881fb8f7043d62331d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 75, 75)            4425      \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 75, 512)           1206272   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 75, 512)           2101248   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 512)               2101248   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 59)                7611      \n",
      "=================================================================\n",
      "Total params: 5,585,028\n",
      "Trainable params: 5,580,603\n",
      "Non-trainable params: 4,425\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    inp = Input(shape=(SEQUENCE_LENGTH, ))\n",
    "    x = Embedding(CHARS_SIZE, 75, trainable=False)(inp)\n",
    "    x = LSTM(512, rseturn_sequences=True,)(x)\n",
    "    x = LSTM(512, return_sequences=True,)(x)\n",
    "    x = LSTM(512,)(x)\n",
    "    x = Dense(256, activation=\"elu\")(x)\n",
    "    x = Dense(128, activation=\"elu\")(x)\n",
    "    outp = Dense(CHARS_SIZE, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=['accuracy'],\n",
    "                 )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5765b4cbab84852306c4e6e4b217d6aec9ac685e"
   },
   "source": [
    "# Checkpoints and Custom Callback\n",
    "We will use 3 callbacks. Checkpoint, EarlyStopping, and a custom TextSample callback. Text sample prints a sample line at the end of every epoch to see how the model is progressing each epoch. For Kaggle, this is less important as you have to commit your code to run this long enough to output results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "b2db6df9efb78f0bc2b741a25aee7f6d769ac98c"
   },
   "outputs": [],
   "source": [
    "filepath=\"model_checkpoint.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                             monitor='loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"loss\",\n",
    "                      mode=\"min\",\n",
    "                      patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "456e33242b7e2591e3caf1f89bd08f9862a6df66"
   },
   "outputs": [],
   "source": [
    "class TextSample(Callback):\n",
    "\n",
    "    def __init__(self):\n",
    "       super(Callback, self).__init__() \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pattern = X_train[700]\n",
    "        outp = []\n",
    "        seed = [charindex[x] for x in pattern]\n",
    "        sample = 'TextSample:' +''.join(seed)+'|'\n",
    "        for t in range(100):\n",
    "          x = np.reshape(pattern, (1, len(pattern)))\n",
    "          pred = self.model.predict(x)\n",
    "          result = np.argmax(pred)\n",
    "          outp.append(result)\n",
    "          pattern = np.append(pattern,result)\n",
    "          pattern = pattern[1:len(pattern)]\n",
    "        outp = [charindex[x] for x in outp]\n",
    "        outp = ''.join(outp)\n",
    "        sample += outp\n",
    "        print(sample)\n",
    "\n",
    "textsample = TextSample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d59dd666b63b5599e3fe414b40ef4ef41ecf2261"
   },
   "source": [
    "\n",
    "# Load Model\n",
    "Load models or weights here. For following up on partially trained models saved by checkpoint. If you want more training time than the 6 hour limit while still using Kaggle, upload the full_train_model.hdf5 output to a private dataset and load it here. Now I *could* do that myself but we are seeing how good we can get it in 6 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c765682ad655aab87450fb758451f3308abe0d64"
   },
   "outputs": [],
   "source": [
    "# model = load_model(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed79d6acea93b121cc1e79a8ade1a5bc584fe423"
   },
   "source": [
    "\n",
    "# Train Model\n",
    "Even with a GPU, this can take a while. As is, I'm setting this notebook to take almost the full 6 hour limit. I have played around with training these types of models for 12 or even 24 hours wit more layers.  However, usually if gotten to roughly around 1.0 loss the generator is good enough to go. Can train almost indefinitely on most models. We are not *really* worried about overfitting. Hypothetically, if the loss gets too low the text might become overfit, which in this case means just copying the text in the most inefficient way. However, it should take an unrealistically long time to get to that point (or maybe just impossible).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6b060047a220fb8afe164789d173dcd1bb3ad85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/26\n"
     ]
    }
   ],
   "source": [
    "model_callbacks = [checkpoint, early, textsample]\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=256,\n",
    "          epochs=26,\n",
    "          verbose=2,\n",
    "          callbacks = model_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "861de6249c9014eeb9bdc379388a34b870b70405"
   },
   "source": [
    "# Save the Model\n",
    "Training is done, save it. This is also a great place to load any pretrained models before generating new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4c9afe74339a6bb070d14bd484dd4d94effef2f"
   },
   "outputs": [],
   "source": [
    "# model = load_model(filepath)\n",
    "model.save_weights(\"full_train_weights.hdf5\")\n",
    "model.save(\"full_train_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ab32b5eb8d64526812d2574506746f8cd434c88"
   },
   "source": [
    "# Generating New Seinfeld Scripts\n",
    "This block generates new text in the style of the input text of TEXT_LENGTH size in characters. It takes a random seed pattern from the training set, predicts the next character, adds it to the end of the pattern, then drops the first character of the pattern and predicts on the new pattern and so forth.\n",
    "\n",
    "Pretty much this text generator *tries* to accurately duplicate the Seinfeld script but inevitably makes errors ,and those errors compound, but is still trained well enough that it ends up making Seinfeld *like* scripts \n",
    "\n",
    "## The Loopbreaker\n",
    "This is simple bit of I came up with while putting this together. Every so many character predictions, the program just changes one of the characters in the pattern to predict on (except the last few, to prevent spelling errors). This causes our model to perceive a slightly different text which causes it to change it's overall predictions slightly too. Without this, even a well trained model might start to repeat itself at some point and get caught in a loop. The loopbreaker can even prevent overfitting or allow under trained models to perform much better. Without a loopbreaker like this, models will need to be trained for many more hours before they can function without looping in on themselves.\n",
    "\n",
    "Changing this value up and down an interesting way to significantly change the output. Setting it high will have more repeated speech, slightly lower might get many line starting the same then vering off into different directions, really low will get lots of varied text but line structures and format might become unstable. Probably keep it somewhere between 1 and 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4dc106ead90a06c6c7b7e7fc6230fe22edc27682"
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "TEXT_LENGTH  = 5000\n",
    "LOOPBREAKER = 3\n",
    "\n",
    "\n",
    "x = np.random.randint(0, len(X_train)-1)\n",
    "pattern = X_train[x]\n",
    "outp = []\n",
    "for t in range(TEXT_LENGTH):\n",
    "  if t % 500 == 0:\n",
    "    print(\"%\"+str((t/TEXT_LENGTH)*100)+\" done\")\n",
    "  \n",
    "  x = np.reshape(pattern, (1, len(pattern)))\n",
    "  pred = model.predict(x, verbose=0)\n",
    "  result = np.argmax(pred)\n",
    "  outp.append(result)\n",
    "  pattern = np.append(pattern,result)\n",
    "  pattern = pattern[1:len(pattern)]\n",
    "  ####loopbreaker####\n",
    "  if t % LOOPBREAKER == 0:\n",
    "    pattern[np.random.randint(0, len(pattern)-10)] = np.random.randint(0, len(charindex)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "85a36d445dac2a5e6865483eb4eacbd39f73686f"
   },
   "source": [
    "# Let's See the Results\n",
    "As you can see, the output is not bad. Text generators like this are pretty good on a line by line basis. Some of the lines seem really plausible as Seinfeld dialogue. Plot and scene structure is off. Different characters show up talking about irrelevant things. In some ways that works comedy. Still, more AI structures are needed to keep track of the plot and such. Anyways, this is the extent of most AI text generation these days without more structured custom code.\n",
    "\n",
    "Final thought: Seinfeld is kind of a tough text to generate from. As a general rule, text generators work better the more abstract the language.  Well, not neccarily *better* just that abstract language makes the imperfect text generation less detectable. This is why stuff like Shakespeare and poetry are popular for AI generation (Also why jazz is popular for AI music generators). People have trouble detecting flaws in less everyday language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93797c8ca1bf0e897c9c7bf8527e2aab2ed452dd"
   },
   "outputs": [],
   "source": [
    "outp = [charindex[x] for x in outp]\n",
    "outp = ''.join(outp)\n",
    "\n",
    "print(outp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db35c3e349e8055f4bc5f6df964bbfc927080cb2"
   },
   "source": [
    "# Save Text Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e7226b8031c3999c35f17fe26cbf6426970b84e4"
   },
   "outputs": [],
   "source": [
    "f = open(\"output_text_sample_1.txt\",\"w\")\n",
    "f.write(outp)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
