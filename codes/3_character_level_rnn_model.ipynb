{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Seinfeld Script Generator\n",
    "\n",
    "Notebook 3: Preprocessing & Modeling - Character Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am a strong believer in that one size doesn't fit for all. There are so many different types of brands out there with various needs, set-ups and available resources, they should not be limited to only one option. And therefore I decided to explore different models which can address their needs at different levels. \n",
    "\n",
    "Upon reading articles and papers from work done by previous data scientists, I decided start with recurrent neural network model which was proven to be unreasonably effective in NLP tasks such as text generation, language translation and speech recognition. RNN is powerful because it creates this loop of updating the weights and states so that the model can have memories. The Long-Short-Term-Memory(LSTM) architecture more specifically, avoid the long-term dependency problem that a simple RNN has trouble with.\n",
    "\n",
    "Here are my thought process on the modeling. As my data size is 4.2MB. It's a good amount to \n",
    "\n",
    "\n",
    "Originally, I tried putting both the directions and dialogue into the text. However, since there are so many directions this ends up making a script that is mostly stuff like \"cut to a picture of a man in the street.\" or \"cut to stock video of a train\" ect. \n",
    "\n",
    "This notebook was originally trained on Kaggle notebook and was heavily inspired by the codes shared by [Patrick DeKelly](https://www.kaggle.com/valkling/pythonicpythonscript4making-seinfeld-scripts) on Kaggle. Takes 30min to train 1 epoch on full data and 4min to train 1 epoch with 500,000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Embedding, LSTM, Dropout, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/for_train.txt\"\n",
    "text = open(path, 'r').read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b6bbcdeeefd738beaaf288fc37f1839d61ebc12"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Next we will prepare an index of every unique character in our text. We are only getting rid of capitalization for simplicity, but still keeping all special characters. This will give us an output that retains the punctuation and format of the original. \n",
    "\n",
    "anything ~1MB+ is great\n",
    "\n",
    "While we could work with the every Seinfeld script, it ends up being a lot of data to go through within the time limit. As such, I added an if block to limit the text data to just the first half million characters. Using more text and training longer is a valid option for improving the output with more training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3592591"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "e16a3a80940fb71572b065010771c5248c834e4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '\\\\', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¿', 'é']\n"
     ]
    }
   ],
   "source": [
    "if len(text) > 500000:\n",
    "    text = text[:500000]\n",
    "\n",
    "char = list(set(text))\n",
    "char.sort() \n",
    "print(char)\n",
    "\n",
    "np.save(\"charindex.npy\", char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jerry: you know, why we\\'re here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about \"we should go out\"? this is what they\\'re talking about...this whole thing, we\\'re all out now, no one is home. not one person here is home, we\\'re all out! there are people tryin\\' to find us, they don\\'t know where we are. (imitates one of these people \"tryin\\' to find us\"; pretends his hand is a phone) \"did you ring?, i can\\'t find him.\" (imitates other person on phone) \"where did he go?\" (the first person again) \"he didn\\'t tell me where he was going\". he must have gone out. you wanna go out: you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...there you\\'re staring around, whatta you do? you go: \"we gotta be getting back\". once you\\'re out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, it\\'s my feeling, you\\'ve gotta go. (pete\\'s luncheonette. jerry and george are sitting at a table.)\\njerry: seems to me, that button is in the worst possible spot. (talking about george\\'s shirt) the second button literally makes or breaks the shirt, look at it: it\\'s too high! it\\'s in no-man\\'s-land, you look like you live with your mother.\\ngeorge: are you through? (kind of irritated)\\njerry: you do of course try on, when you buy?\\ngeorge: yes, it was purple, i liked it, i don\\'t actually recall considering the buttons.\\njerry: oh, you don\\'t recall?\\ngeorge: (pretends he\\'s talking into a microphone) uh, no, not at this time.\\njerry: well, senator, i just like to know, what you knew and when you knew it. (a waitress approaches the table)\\nwaitress: mister seinfeld. (she pours coffee in his cup) mister costanza. (she wants to pour coffee, but george stops her)\\ngeorge: are, are you sure this is decaf? where\\'s the orange indicator?\\nwaitress: it\\'s missing'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "39df6021cb3006a38b59108b116e9514a8015fa8"
   },
   "source": [
    "# Create Sequences\n",
    "In a nutshell, this model will look at the last 75 characters in the script and attempt to predict the 76th. Our X variable will be a 75 character sequence and our Y variable will be the 76th character. This block chops the text data into such sequences of characters. \n",
    "\n",
    "Note that this part also tokenizes the characters, which is to say it replaces each character with a number that corresponds to it's index in charindex. This is why it is important to save a copy of the charindex with your model just in case. We will need it to decode our predictions later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "ff8898a2997216697ffdb6ab6b6a98e77355b431"
   },
   "outputs": [],
   "source": [
    "maxlen = 75\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(0, len(text)-maxlen, 1 ): \n",
    "    X = text[i:i + maxlen]\n",
    "    y = text[i + maxlen]\n",
    "    X_train.append([char.index(x) for x in X])\n",
    "    y_train.append(char.index(y))\n",
    "\n",
    "X_train = np.reshape(X_train, (len(X_train), maxlen))\n",
    "y_train = np_utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0847d548defb45832406ac64fa3873749fe2b5d"
   },
   "source": [
    "# Create the Model\n",
    "The model uses 3 LSTMs stacked on top of each. Adding another LSTM layer and/or running it a lot longer or in multiple session will give better results. However, the 3 LSTM should do fine in 6 hour and adding the loopbreaker to our code later will make even under trained models give good results. Also note that we are using CuDNNLSTMs. If you don't know what that is, it is a special LSTM layer specially made for NIVDA GPUs. These function the same as regular LSTM layers but are automatically optimised for the GPU. You lose some customization with these layers but they work roughly twice as fast as regular LSTMs layers if conditions are right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "f6498ca0985caf5a773a5e881fb8f7043d62331d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 75, 75)            4425      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 75, 512)           1204224   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 75, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 59)                7611      \n",
      "=================================================================\n",
      "Total params: 5,578,884\n",
      "Trainable params: 5,574,459\n",
      "Non-trainable params: 4,425\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Input(shape=(maxlen, )))\n",
    "    model.add(Embedding(len(char), 75, trainable=False))\n",
    "    \n",
    "    model.add(LSTM(512, return_sequences=True,))\n",
    "    model.add(LSTM(512, return_sequences=True,))\n",
    "    model.add(LSTM(512,))\n",
    "    \n",
    "    model.add(Dense(256, activation=\"elu\"))\n",
    "    model.add(Dense(128, activation=\"elu\"))\n",
    "    model.add(Dense(len(char), activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5765b4cbab84852306c4e6e4b217d6aec9ac685e"
   },
   "source": [
    "# Checkpoints and Custom Callback\n",
    "We will use 3 callbacks. Checkpoint, EarlyStopping, and a custom TextSample callback. Text sample prints a sample line at the end of every epoch to see how the model is progressing each epoch. For Kaggle, this is less important as you have to commit your code to run this long enough to output results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "b2db6df9efb78f0bc2b741a25aee7f6d769ac98c"
   },
   "outputs": [],
   "source": [
    "filepath=\"../assets/char_based/model_checkpoint.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                             monitor='loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"loss\",\n",
    "                      mode=\"min\",\n",
    "                      patience=1)\n",
    "\n",
    "callbacks = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed79d6acea93b121cc1e79a8ade1a5bc584fe423"
   },
   "source": [
    "Even with a GPU, this can take a while. As is, I'm setting this notebook to take almost the full 6 hour limit. I have played around with training these types of models for 12 or even 24 hours wit more layers.  However, usually if gotten to roughly around 1.0 loss the generator is good enough to go. Can train almost indefinitely on most models. We are not *really* worried about overfitting. Hypothetically, if the loss gets too low the text might become overfit, which in this case means just copying the text in the most inefficient way. However, it should take an unrealistically long time to get to that point (or maybe just impossible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6b060047a220fb8afe164789d173dcd1bb3ad85"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=256,\n",
    "          epochs=26,\n",
    "          verbose=1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4c9afe74339a6bb070d14bd484dd4d94effef2f"
   },
   "outputs": [],
   "source": [
    "# model = load_model(filepath)\n",
    "model.save_weights(\"full_train_weights.hdf5\")\n",
    "model.save(\"full_train_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ab32b5eb8d64526812d2574506746f8cd434c88"
   },
   "source": [
    "# Generating New Seinfeld Scripts\n",
    "This block generates new text in the style of the input text of TEXT_LENGTH size in characters. It takes a random seed pattern from the training set, predicts the next character, adds it to the end of the pattern, then drops the first character of the pattern and predicts on the new pattern and so forth.\n",
    "\n",
    "Pretty much this text generator *tries* to accurately duplicate the Seinfeld script but inevitably makes errors ,and those errors compound, but is still trained well enough that it ends up making Seinfeld *like* scripts \n",
    "\n",
    "\n",
    "This is simple bit of I came up with while putting this together. Every so many character predictions, the program just changes one of the characters in the pattern to predict on (except the last few, to prevent spelling errors). This causes our model to perceive a slightly different text which causes it to change it's overall predictions slightly too. Without this, even a well trained model might start to repeat itself at some point and get caught in a loop. The loopbreaker can even prevent overfitting or allow under trained models to perform much better. Without a loopbreaker like this, models will need to be trained for many more hours before they can function without looping in on themselves.\n",
    "\n",
    "Changing this value up and down an interesting way to significantly change the output. Setting it high will have more repeated speech, slightly lower might get many line starting the same then vering off into different directions, really low will get lots of varied text but line structures and format might become unstable. Probably keep it somewhere between 1 and 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4dc106ead90a06c6c7b7e7fc6230fe22edc27682"
   },
   "outputs": [],
   "source": [
    "next_letters  = 5000\n",
    "\n",
    "x = np.random.randint(0, len(X_train)-1)\n",
    "pattern = X_train[x]\n",
    "generated = []\n",
    "for t in range(next_letters):\n",
    "    if t % 500 == 0:\n",
    "        print(\"%\"+str((t/next_letters)*100)+\" done\")\n",
    "  \n",
    "    x = np.reshape(pattern, (1, len(pattern)))\n",
    "    pred = model.predict(x)\n",
    "    result = np.argmax(pred)\n",
    "    generated.append(result)\n",
    "    pattern = np.append(pattern,result)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "85a36d445dac2a5e6865483eb4eacbd39f73686f"
   },
   "source": [
    "As you can see, the output is not bad. Text generators like this are pretty good on a line by line basis. Some of the lines seem really plausible as Seinfeld dialogue. Plot and scene structure is off. Different characters show up talking about irrelevant things. In some ways that works comedy. Still, more AI structures are needed to keep track of the plot and such. Anyways, this is the extent of most AI text generation these days without more structured custom code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = [char[x] for x in generated]\n",
    "generated = ''.join(generated)\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e7226b8031c3999c35f17fe26cbf6426970b84e4"
   },
   "outputs": [],
   "source": [
    "f = open(\"../texts/char_level.txt\",\"w\")\n",
    "f.write(outp)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
