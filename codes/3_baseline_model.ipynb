{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Capstone Project\n",
    "\n",
    "Notebook 3: Preprocessing & Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.utils as ku "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/for_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sepearting Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_punch(text):\n",
    "    punch = ['...', '.', '[', ']', '(', ')', ';', ':', \"'\", '/', '\"', ',', '?', '*', '!', '-', '$', '%', '&']\n",
    "    for i in punch:\n",
    "        text = text.replace(i, ' ' + i + ' ')\n",
    "    return text\n",
    "\n",
    "df = df['char_line'].map(seperate_punch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    jerry :  you know ,  why we ' re here ?  to be out ,  this is out  .  .  .  and out is one of the single most enjoyable experiences of life .  people  .  .  .  did you ever hear people talking about  \" we should go out \"  ?  this is what they ' re talking about  .  .  .  this whole thing ,  we ' re all out now ,  no one is home .  not one person here is home ,  we ' re all out !  there are people tryin '  to find us ,  they don ' t know where we are .   ( imitates one of these people  \" tryin '  to find us \"  ;  pretends his hand is a phone )   \" did you ring ?  ,  i can ' t find him .  \"   ( imitates other person on phone )   \" where did he go ?  \"   ( the first person again )   \" he didn ' t tell me where he was going \"  .  he must have gone out .  you wanna go out :  you get ready ,  you pick out the clothes ,  right ?  you take the shower ,  you get all ready ,  get the cash ,  get your friends ,  the car ,  the spot ,  the reservation  .  .  .  there you ' re staring around ,  whatta you do ?  you go :   \" we gotta be getting back \"  .  once you ' re out ,  you wanna get back !  you wanna go to sleep ,  you wanna get up ,  you wanna go out again tomorrow ,  right ?  where ever you are in life ,  it ' s my feeling ,  you ' ve gotta go .   ( pete ' s luncheonette .  jerry and george are sitting at a table .  ) \n",
       "1    jerry :  seems to me ,  that button is in the worst possible spot .   ( talking about george ' s shirt )  the second button literally makes or breaks the shirt ,  look at it :  it ' s too high !  it ' s in no - man ' s - land ,  you look like you live with your mother .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "2    george :  are you through ?   ( kind of irritated )                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "3    jerry :  you do of course try on ,  when you buy ?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "4    george :  yes ,  it was purple ,  i liked it ,  i don ' t actually recall considering the buttons .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "Name: char_line, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for text in df:\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18647 unique tokens\n",
      "Shape of data tensor: (50639, 200)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 200\n",
    "max_words = 10000\n",
    "batch_size = 32\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens')\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print(f'Shape of data tensor: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 703,\n",
       " 8,\n",
       " 23,\n",
       " 15,\n",
       " 806,\n",
       " 19,\n",
       " 18,\n",
       " 3,\n",
       " 1062,\n",
       " 973,\n",
       " 912,\n",
       " 174,\n",
       " 58,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 384,\n",
       " 3,\n",
       " 249,\n",
       " 806,\n",
       " 2629,\n",
       " 321,\n",
       " 170,\n",
       " 1330,\n",
       " 3,\n",
       " 384,\n",
       " 79,\n",
       " 38,\n",
       " 10,\n",
       " 10,\n",
       " 1,\n",
       " 6,\n",
       " 135,\n",
       " 554,\n",
       " 10,\n",
       " 1,\n",
       " 6,\n",
       " 18,\n",
       " 24,\n",
       " 113,\n",
       " 1,\n",
       " 6,\n",
       " 2241,\n",
       " 4,\n",
       " 79,\n",
       " 50,\n",
       " 4,\n",
       " 432,\n",
       " 28,\n",
       " 55,\n",
       " 377]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  16,   46,    1, ...,   38,    7,  192],\n",
       "       [   0,    0,    0, ...,   28,   55,  377],\n",
       "       [   0,    0,    0, ...,  220,   17, 1954],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   17,   30, 1921],\n",
       "       [   0,    0,    0, ...,   17,   55,  219],\n",
       "       [   0,    0,    0, ...,   64,   89,   21]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"jerry :  seems to me ,  that button is in the worst possible spot .   ( talking about george ' s shirt )  the second button literally makes or breaks the shirt ,  look at it :  it ' s too high !  it ' s in no - man ' s - land ,  you look like you live with your mother . \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 Simple Modeling - Test Water\n",
    "https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\n",
    "& Francoise Chollet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(data, maxlen):\n",
    "    \n",
    "    X, y = data[:,:-1], data[:,-1]\n",
    "    y = ku.to_categorical(y, num_classes=max_words)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_training_data(data, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50639, 199)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1603/1603 [==============================] - 217s 135ms/step - loss: 7.0584\n",
      "Epoch 2/100\n",
      "1603/1603 [==============================] - 206s 129ms/step - loss: 6.6017\n",
      "Epoch 3/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 6.4238\n",
      "Epoch 4/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 6.3002\n",
      "Epoch 5/100\n",
      "1603/1603 [==============================] - 210s 131ms/step - loss: 6.2068\n",
      "Epoch 6/100\n",
      "1603/1603 [==============================] - 209s 131ms/step - loss: 6.1631\n",
      "Epoch 7/100\n",
      "1603/1603 [==============================] - 217s 135ms/step - loss: 6.5028\n",
      "Epoch 8/100\n",
      "1603/1603 [==============================] - 212s 132ms/step - loss: 6.7308\n",
      "Epoch 9/100\n",
      "1603/1603 [==============================] - 208s 129ms/step - loss: 6.8238\n",
      "Epoch 10/100\n",
      "1603/1603 [==============================] - 206s 128ms/step - loss: 6.9056\n",
      "Epoch 11/100\n",
      "1603/1603 [==============================] - 211s 131ms/step - loss: 6.9744\n",
      "Epoch 12/100\n",
      "1603/1603 [==============================] - 216s 135ms/step - loss: 7.0305\n",
      "Epoch 13/100\n",
      "1603/1603 [==============================] - 212s 132ms/step - loss: 7.0745\n",
      "Epoch 14/100\n",
      "1603/1603 [==============================] - 208s 129ms/step - loss: 7.1048\n",
      "Epoch 15/100\n",
      "1603/1603 [==============================] - 211s 131ms/step - loss: 7.1284\n",
      "Epoch 16/100\n",
      "1603/1603 [==============================] - 216s 135ms/step - loss: 7.1462\n",
      "Epoch 17/100\n",
      "1603/1603 [==============================] - 214s 133ms/step - loss: 7.1587\n",
      "Epoch 18/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.1666\n",
      "Epoch 19/100\n",
      "1603/1603 [==============================] - 210s 131ms/step - loss: 7.1717\n",
      "Epoch 20/100\n",
      "1603/1603 [==============================] - 214s 133ms/step - loss: 7.1760\n",
      "Epoch 21/100\n",
      "1603/1603 [==============================] - 212s 133ms/step - loss: 7.1761\n",
      "Epoch 22/100\n",
      "1603/1603 [==============================] - 214s 133ms/step - loss: 7.1744\n",
      "Epoch 23/100\n",
      "1603/1603 [==============================] - 217s 136ms/step - loss: 7.1706\n",
      "Epoch 24/100\n",
      "1603/1603 [==============================] - 213s 133ms/step - loss: 7.1704\n",
      "Epoch 25/100\n",
      "1603/1603 [==============================] - 212s 132ms/step - loss: 7.1658\n",
      "Epoch 26/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.1640\n",
      "Epoch 27/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.1631\n",
      "Epoch 28/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.1598\n",
      "Epoch 29/100\n",
      "1603/1603 [==============================] - 209s 130ms/step - loss: 7.1562\n",
      "Epoch 30/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.1562\n",
      "Epoch 31/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.1507\n",
      "Epoch 32/100\n",
      "1603/1603 [==============================] - 210s 131ms/step - loss: 7.1463\n",
      "Epoch 33/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.1434\n",
      "Epoch 34/100\n",
      "1603/1603 [==============================] - 206s 129ms/step - loss: 7.1431\n",
      "Epoch 35/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.1391\n",
      "Epoch 36/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.1340\n",
      "Epoch 37/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.1318\n",
      "Epoch 38/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.1337\n",
      "Epoch 39/100\n",
      "1603/1603 [==============================] - 209s 131ms/step - loss: 7.1273\n",
      "Epoch 40/100\n",
      "1603/1603 [==============================] - 206s 129ms/step - loss: 7.1248\n",
      "Epoch 41/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.1226\n",
      "Epoch 42/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.1226\n",
      "Epoch 43/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.1204\n",
      "Epoch 44/100\n",
      "1603/1603 [==============================] - 209s 130ms/step - loss: 7.1150\n",
      "Epoch 45/100\n",
      "1603/1603 [==============================] - 209s 131ms/step - loss: 7.1139\n",
      "Epoch 46/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.1124\n",
      "Epoch 47/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.1079\n",
      "Epoch 48/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.1067\n",
      "Epoch 49/100\n",
      "1603/1603 [==============================] - 206s 128ms/step - loss: 7.1064\n",
      "Epoch 50/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.1044\n",
      "Epoch 51/100\n",
      "1603/1603 [==============================] - 210s 131ms/step - loss: 7.1037\n",
      "Epoch 52/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.1015\n",
      "Epoch 53/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.0997\n",
      "Epoch 54/100\n",
      "1603/1603 [==============================] - 209s 130ms/step - loss: 7.0964\n",
      "Epoch 56/100\n",
      "1603/1603 [==============================] - 209s 130ms/step - loss: 7.0953\n",
      "Epoch 57/100\n",
      "1603/1603 [==============================] - 210s 131ms/step - loss: 7.0935\n",
      "Epoch 58/100\n",
      "1603/1603 [==============================] - 209s 130ms/step - loss: 7.0936\n",
      "Epoch 59/100\n",
      "1603/1603 [==============================] - 209s 130ms/step - loss: 7.0935\n",
      "Epoch 60/100\n",
      "1603/1603 [==============================] - 210s 131ms/step - loss: 7.0904\n",
      "Epoch 61/100\n",
      "1603/1603 [==============================] - 212s 132ms/step - loss: 7.0921\n",
      "Epoch 62/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.0891\n",
      "Epoch 63/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.0849\n",
      "Epoch 64/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.0839\n",
      "Epoch 65/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.0834\n",
      "Epoch 66/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.0866\n",
      "Epoch 67/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.0834\n",
      "Epoch 68/100\n",
      "1603/1603 [==============================] - 209s 130ms/step - loss: 7.0802\n",
      "Epoch 69/100\n",
      "1603/1603 [==============================] - 212s 132ms/step - loss: 7.0821\n",
      "Epoch 70/100\n",
      "1603/1603 [==============================] - 214s 134ms/step - loss: 7.0822\n",
      "Epoch 71/100\n",
      "1603/1603 [==============================] - 209s 130ms/step - loss: 7.0808\n",
      "Epoch 72/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.0816\n",
      "Epoch 73/100\n",
      "1603/1603 [==============================] - 206s 129ms/step - loss: 7.0792\n",
      "Epoch 74/100\n",
      "1603/1603 [==============================] - 210s 131ms/step - loss: 7.0784\n",
      "Epoch 75/100\n",
      "1603/1603 [==============================] - 209s 131ms/step - loss: 7.0784\n",
      "Epoch 76/100\n",
      "1603/1603 [==============================] - 212s 132ms/step - loss: 7.0776\n",
      "Epoch 77/100\n",
      "1603/1603 [==============================] - 211s 132ms/step - loss: 7.0777\n",
      "Epoch 78/100\n",
      "1603/1603 [==============================] - 218s 136ms/step - loss: 7.0770\n",
      "Epoch 79/100\n",
      "1603/1603 [==============================] - 218s 136ms/step - loss: 7.0766\n",
      "Epoch 80/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.0739\n",
      "Epoch 81/100\n",
      "1603/1603 [==============================] - 205s 128ms/step - loss: 7.0750\n",
      "Epoch 82/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.0751\n",
      "Epoch 83/100\n",
      "1603/1603 [==============================] - 210s 131ms/step - loss: 7.0728\n",
      "Epoch 84/100\n",
      "1603/1603 [==============================] - 209s 130ms/step - loss: 7.0718\n",
      "Epoch 85/100\n",
      "1603/1603 [==============================] - 212s 132ms/step - loss: 7.0707\n",
      "Epoch 86/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.0687\n",
      "Epoch 87/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.0701\n",
      "Epoch 88/100\n",
      "1603/1603 [==============================] - 210s 131ms/step - loss: 7.0695\n",
      "Epoch 89/100\n",
      "1603/1603 [==============================] - 206s 129ms/step - loss: 7.0675\n",
      "Epoch 90/100\n",
      "1603/1603 [==============================] - 206s 128ms/step - loss: 7.0713\n",
      "Epoch 91/100\n",
      "1603/1603 [==============================] - 208s 130ms/step - loss: 7.0697\n",
      "Epoch 92/100\n",
      "1603/1603 [==============================] - 205s 128ms/step - loss: 7.0634\n",
      "Epoch 93/100\n",
      "1603/1603 [==============================] - 209s 130ms/step - loss: 7.0672\n",
      "Epoch 94/100\n",
      "1603/1603 [==============================] - 213s 133ms/step - loss: 7.0664\n",
      "Epoch 95/100\n",
      "1603/1603 [==============================] - 210s 131ms/step - loss: 7.0633\n",
      "Epoch 96/100\n",
      "1603/1603 [==============================] - 209s 130ms/step - loss: 7.0646\n",
      "Epoch 97/100\n",
      "1603/1603 [==============================] - 206s 128ms/step - loss: 7.0671\n",
      "Epoch 98/100\n",
      "1603/1603 [==============================] - 209s 130ms/step - loss: 7.0652\n",
      "Epoch 99/100\n",
      "1603/1603 [==============================] - 207s 129ms/step - loss: 7.0659\n",
      "Epoch 100/100\n",
      "1603/1603 [==============================] - 209s 130ms/step - loss: 7.0658\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_words, output_dim=maxlen))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(max_words, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "history = model.fit(X, y,\n",
    "                   epochs=100,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, maxlen):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=maxlen-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"soup nazi do it is it up there now ' s you doing here now ' s him huh huh huh right now ' s you doing here again now jerry yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('soup nazi', 100, model, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model I used keras tokenizer to tokenize the data, and I used SimpleRNN to test out: the result is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - character based's way with generator\n",
    "https://github.com/TannerGilbert/Tutorials/blob/master/Keras-Tutorials/4.%20LSTM%20Text%20Generation/Keras%20LSTM%20Text%20Generation.ipynb\n",
    "https://www.kaggle.com/valkling/pythonicpythonscript4making-seinfeld-scripts\n",
    "char RNN by karparchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - TEXTRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - GPT-2\n",
    "\n",
    "shared by Hilary\n",
    "https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=LdpZQXknFNY3\n",
    "\n",
    "Script Buddy V2\n",
    "https://github.com/cdpierse/script_buddy_v2/blob/master/script_buddy/script_generation.ipynb\n",
    "\n",
    "Teach GPT-2 sense of humor\n",
    "https://github.com/mf1024/Transformers/blob/master/Teaching%20GPT-2%20a%20sense%20of%20humor.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
