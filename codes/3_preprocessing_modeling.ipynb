{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Capstone Project\n",
    "\n",
    "Notebook 3: Preprocessing & Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.utils as ku "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jerry: you know, why we're here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about \"we should go out\"? this is what they're talking about...this whole thing, we're all out now, no one is home. not one person here is home, we're all out! there are people tryin' to find us, they don't know where we are. (imitates one of these people \"tryin' to find us\"; pretends his hand is a phone) \"did you ring?, i can't find him.\" (imitates other person on phone) \"where did he go?\" (the first person again) \"he didn't tell me where he was going\". he must have gone out. you wanna go out: you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...there you're staring around, whatta you do? you go: \"we gotta be getting back\". once you're out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, it's my feeling, you've gotta go. (pete's luncheonette. jerry and george are sitting at a table.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jerry: seems to me, that button is in the worst possible spot. (talking about george's shirt) the second button literally makes or breaks the shirt, look at it: it's too high! it's in no-man's-land, you look like you live with your mother.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>george: are you through? (kind of irritated)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jerry: you do of course try on, when you buy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>george: yes, it was purple, i liked it, i don't actually recall considering the buttons.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   char_line\n",
       "0  jerry: you know, why we're here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about \"we should go out\"? this is what they're talking about...this whole thing, we're all out now, no one is home. not one person here is home, we're all out! there are people tryin' to find us, they don't know where we are. (imitates one of these people \"tryin' to find us\"; pretends his hand is a phone) \"did you ring?, i can't find him.\" (imitates other person on phone) \"where did he go?\" (the first person again) \"he didn't tell me where he was going\". he must have gone out. you wanna go out: you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...there you're staring around, whatta you do? you go: \"we gotta be getting back\". once you're out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, it's my feeling, you've gotta go. (pete's luncheonette. jerry and george are sitting at a table.)\n",
       "1  jerry: seems to me, that button is in the worst possible spot. (talking about george's shirt) the second button literally makes or breaks the shirt, look at it: it's too high! it's in no-man's-land, you look like you live with your mother.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "2  george: are you through? (kind of irritated)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "3  jerry: you do of course try on, when you buy?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "4  george: yes, it was purple, i liked it, i don't actually recall considering the buttons.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/for_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sepearting Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_punch(text):\n",
    "    punch = ['...', '.', '[', ']', '(', ')', ';', ':', \"'\", '/', '\"', ',', '?', '*', '!', '-', '$', '%', '&', '\\n']\n",
    "    for i in punch:\n",
    "        text = text.replace(i, ' ' + i + ' ')\n",
    "    return text\n",
    "\n",
    "df['char_line'] = df['char_line'].map(seperate_punch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for text in df['char_line']:\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18647 unique tokens\n",
      "Shape of data tensor: (51292, 200)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 200\n",
    "max_words = 10000\n",
    "batch_size = 32\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens')\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print(f'Shape of data tensor: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 710,\n",
       " 8,\n",
       " 23,\n",
       " 15,\n",
       " 819,\n",
       " 19,\n",
       " 18,\n",
       " 3,\n",
       " 1072,\n",
       " 987,\n",
       " 907,\n",
       " 170,\n",
       " 59,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 388,\n",
       " 3,\n",
       " 249,\n",
       " 819,\n",
       " 2641,\n",
       " 324,\n",
       " 169,\n",
       " 1339,\n",
       " 3,\n",
       " 388,\n",
       " 79,\n",
       " 39,\n",
       " 10,\n",
       " 10,\n",
       " 1,\n",
       " 6,\n",
       " 134,\n",
       " 557,\n",
       " 10,\n",
       " 1,\n",
       " 6,\n",
       " 18,\n",
       " 24,\n",
       " 112,\n",
       " 1,\n",
       " 6,\n",
       " 2254,\n",
       " 4,\n",
       " 79,\n",
       " 50,\n",
       " 4,\n",
       " 423,\n",
       " 28,\n",
       " 55,\n",
       " 379]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  16,   46,    1, ...,   39,    7,  191],\n",
       "       [   0,    0,    0, ...,   28,   55,  379],\n",
       "       [   0,    0,    0, ...,  220,   17, 1968],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   17,   30, 1934],\n",
       "       [   0,    0,    0, ...,   17,   55,  219],\n",
       "       [   0,    0,    0, ...,   64,   88,   21]], dtype=int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"jerry :  seems to me ,  that button is in the worst possible spot .   ( talking about george ' s shirt )  the second button literally makes or breaks the shirt ,  look at it :  it ' s too high !  it ' s in no - man ' s - land ,  you look like you live with your mother . \""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 Simple Modeling - Test Water\n",
    "https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\n",
    "& Francoise Challot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(data, maxlen):\n",
    "    \n",
    "    X, y = data[:,:-1], data[:,-1]\n",
    "    y = ku.to_categorical(y, num_classes=max_words)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_training_data(data, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1603/1603 [==============================] - 202s 126ms/step - loss: 7.0511\n",
      "Epoch 2/30\n",
      "1603/1603 [==============================] - 202s 126ms/step - loss: 6.5714\n",
      "Epoch 3/30\n",
      "1603/1603 [==============================] - 201s 125ms/step - loss: 6.3978\n",
      "Epoch 4/30\n",
      "1603/1603 [==============================] - 201s 125ms/step - loss: 6.2853\n",
      "Epoch 5/30\n",
      "1603/1603 [==============================] - 201s 125ms/step - loss: 6.1975\n",
      "Epoch 6/30\n",
      "1603/1603 [==============================] - 202s 126ms/step - loss: 6.1544\n",
      "Epoch 7/30\n",
      "1603/1603 [==============================] - 199s 124ms/step - loss: 6.2568\n",
      "Epoch 8/30\n",
      "1603/1603 [==============================] - 202s 126ms/step - loss: 6.6150\n",
      "Epoch 9/30\n",
      "1603/1603 [==============================] - 201s 125ms/step - loss: 6.8853\n",
      "Epoch 10/30\n",
      "1603/1603 [==============================] - 202s 126ms/step - loss: 6.9794\n",
      "Epoch 11/30\n",
      "1603/1603 [==============================] - 202s 126ms/step - loss: 7.0272\n",
      "Epoch 12/30\n",
      "1603/1603 [==============================] - 202s 126ms/step - loss: 7.0724\n",
      "Epoch 13/30\n",
      "1603/1603 [==============================] - 203s 127ms/step - loss: 7.1098\n",
      "Epoch 14/30\n",
      "1603/1603 [==============================] - 204s 127ms/step - loss: 7.1338\n",
      "Epoch 15/30\n",
      "1603/1603 [==============================] - 203s 127ms/step - loss: 7.1525\n",
      "Epoch 16/30\n",
      "1603/1603 [==============================] - 202s 126ms/step - loss: 7.1670\n",
      "Epoch 17/30\n",
      "1603/1603 [==============================] - 203s 127ms/step - loss: 7.1757\n",
      "Epoch 18/30\n",
      "1603/1603 [==============================] - 204s 127ms/step - loss: 7.1782\n",
      "Epoch 19/30\n",
      "1603/1603 [==============================] - 205s 128ms/step - loss: 7.1823\n",
      "Epoch 20/30\n",
      "1603/1603 [==============================] - 201s 126ms/step - loss: 7.1850\n",
      "Epoch 21/30\n",
      "1603/1603 [==============================] - 202s 126ms/step - loss: 7.1854\n",
      "Epoch 22/30\n",
      "1603/1603 [==============================] - 202s 126ms/step - loss: 7.1831\n",
      "Epoch 23/30\n",
      "1603/1603 [==============================] - 203s 126ms/step - loss: 7.1797\n",
      "Epoch 24/30\n",
      "1603/1603 [==============================] - 204s 127ms/step - loss: 7.1796\n",
      "Epoch 25/30\n",
      "1603/1603 [==============================] - 201s 125ms/step - loss: 7.1733\n",
      "Epoch 26/30\n",
      "1603/1603 [==============================] - 202s 126ms/step - loss: 7.1712\n",
      "Epoch 27/30\n",
      "1603/1603 [==============================] - 201s 126ms/step - loss: 7.1660\n",
      "Epoch 28/30\n",
      "1603/1603 [==============================] - 202s 126ms/step - loss: 7.1622\n",
      "Epoch 29/30\n",
      "1603/1603 [==============================] - 201s 125ms/step - loss: 7.1589\n",
      "Epoch 30/30\n",
      "1603/1603 [==============================] - 202s 126ms/step - loss: 7.1568\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_words, output_dim=maxlen))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(max_words, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "history = model.fit(X, y,\n",
    "                   epochs=30,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, maxlen):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=maxlen-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soup nazi you know that again you going on here again about it is you know george yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah yeah'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('soup nazi', 100, model, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model I used keras tokenizer to tokenize the data, and I used SimpleRNN to test out: the result is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - manually tokenizing credit \n",
    "credit https://medium.com/coloredfeather/generating-a-tv-script-using-recurrent-neural-networks-dd0a645e97e7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - russian guy's way\n",
    "https://github.com/TannerGilbert/Tutorials/blob/master/Keras-Tutorials/4.%20LSTM%20Text%20Generation/Keras%20LSTM%20Text%20Generation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - Use generator\n",
    "Francoise textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 - GPT-2\n",
    "\n",
    "shared by Hilary\n",
    "https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=LdpZQXknFNY3\n",
    "\n",
    "Script Buddy V2\n",
    "https://github.com/cdpierse/script_buddy_v2/blob/master/script_buddy/script_generation.ipynb\n",
    "\n",
    "Teach GPT-2 sense of humor\n",
    "https://github.com/mf1024/Transformers/blob/master/Teaching%20GPT-2%20a%20sense%20of%20humor.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
